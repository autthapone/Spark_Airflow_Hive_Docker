version: '3.7'
services:

    postgres:                                           # create postgres container
        image: postgres:9.6
        container_name: postgres_container
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow

    airflow:                                            # create airflow container
        build: './airflow_docker'                       # construct the container along the Dockerfile in this folder
        container_name: airflow_container
        restart: always
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
        volumes:                                        # mount the following local folders
            - ./dags:/usr/local/airflow/dags
            - ./data:/usr/local/airflow/data
        ports:
            - "8080:8080"                               # expose port
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    zookeeper:                                          # create zookeeper container
        image: wurstmeister/zookeeper
        container_name: zookeeper_container
        ports:
            - "2181:2181"                               # expose port
        networks:
            net_pet:
                ipv4_address: 172.27.1.15

    kafka:                                              # create an instance of a Kafka broker in a container
        image: wurstmeister/kafka
        container_name: kafka_container
        ports:
            - "9092:9092"                               # expose port
        environment:
            KAFKA_ADVERTISED_HOST_NAME: kafka                               # specify the docker host IP at which other containers can reach the broker
            KAFKA_CREATE_TOPICS: "transactions:1:1,locations:1:1"           # create a 2 topics  with 1 partition and 1 replica
            KAFKA_ADVERTISED_HOST_NAME: 172.27.1.16
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181                         # specify where the broker can reach Zookeeper
            KAFKA_LISTENERS: PLAINTEXT://kafka:9092                         # the list of addresses on which the Kafka broker will listen on for incoming connections.
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092              # Kafka sends the value of this variable to clients during their connection. After receiving that value, the clients use it for sending/consuming records to/from the Kafka broker.y connect to it.
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock
        networks:
            net_pet:
                ipv4_address: 172.27.1.16  

    namenode:
        image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
        container_name: namenode
        volumes:
            - /tmp/hdfs/namenode:/hadoop/dfs/name
        environment:
            - CLUSTER_NAME=test
        env_file:
            - ./hadoop-hive.env
        ports:
            - "50070:50070"
        networks:
            net_pet:
            ipv4_address: 172.27.1.5
        
    datanode:
        image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
        container_name: datanode
        volumes:
            - /tmp/hdfs/datanode:/hadoop/dfs/data
            - ./bank:/bank
        env_file:
            - ./hadoop-hive.env
        environment:
            SERVICE_PRECONDITION: "namenode:50070"
        depends_on:
            - namenode
        ports:
            - "50075:50075"
        networks:
            net_pet:
            ipv4_address: 172.27.1.6
        
    hive-server:
        image: bde2020/hive:2.3.2-postgresql-metastore
        container_name: hive-server
        env_file:
            - ./hadoop-hive.env
        environment:
            HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
            SERVICE_PRECONDITION: "hive-metastore:9083"
        ports:
            - "10000:10000"
        depends_on:
            - hive-metastore
        networks:
            net_pet:
            ipv4_address: 172.27.1.7
        
    hive-metastore:
        image: bde2020/hive:2.3.2-postgresql-metastore
        container_name: hive-metastore
        env_file:
            - ./hadoop-hive.env
        command: /opt/hive/bin/hive --service metastore
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075 hive-metastore-postgresql:5432"
        ports:
            - "9083:9083"
        depends_on:
            - hive-metastore-postgresql
        networks:
            net_pet:
            ipv4_address: 172.27.1.8
        
    hive-metastore-postgresql:
        image: bde2020/hive-metastore-postgresql:2.3.0
        container_name: hive-metastore-postgresql
        depends_on:
            - datanode
        networks:
            net_pet:
            ipv4_address: 172.27.1.9
        
    spark-master:
        image: bde2020/spark-master:2.4.0-hadoop2.7
        container_name: spark-master
        ports:
            - 8080:8080
            - 7077:7077
        environment:
            - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
        env_file:
            - ./hadoop-hive.env
        networks:
            net_pet:
            ipv4_address: 172.27.1.10
            
    spark-worker:
        image: bde2020/spark-worker:2.4.0-hadoop2.7
        container_name: spark-worker
        depends_on:
            - spark-master
        environment:
            - SPARK_MASTER=spark://spark-master:7077
            - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
            - HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore/metastore
        depends_on:
            - spark-master
        ports:
            - 8081:8081
        env_file:
            - ./hadoop-hive.env
        networks:
            net_pet:
            ipv4_address: 172.27.1.11
    
    zeppelin:
        image: openkbs/docker-spark-bde2020-zeppelin
        container_name: zeppelin
        environment:
            CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
            SPARK_MASTER: "spark://spark-master:7077"
            MASTER: "spark://spark-master:7077"
            SPARK_MASTER_URL: spark://spark-master:7077
            ZEPPELIN_PORT: 8080
            ZEPPELIN_JAVA_OPTS:
            -Dspark.driver.memory=1g
            -Dspark.executor.memory=2g
        ports:
            - 19090:8080
        env_file:
            - ./hadoop-hive.env
        volumes:
            - /tmp/simple-demo/zeppelin/data:/usr/lib/zeppelin/data:rw
            - /tmp/simple-demo/zeppelin/notebook:/usr/lib/zeppelin/notebook:rw
        command: /usr/lib/zeppelin/bin/zeppelin.sh
        networks:
            net_pet:
            ipv4_address: 172.27.1.12
    
    hue:
        image: gethue/hue:20191107-135001
        hostname: hue
        container_name: hue
        dns: 8.8.8.8
        ports:
        - "8888:8888"
        volumes:
        - ./hue-overrides.ini:/usr/share/hue/desktop/conf/z-hue.ini
        depends_on:
        - "database"
        networks:
        net_pet:
            ipv4_address: 172.27.1.13
    
    database:
        image: mysql:5.7
        container_name: database
        ports:
            - "33061:3306"
        command: --init-file /data/application/init.sql
        volumes:
            - /tmp/mysql/data:/var/lib/mysql
            - ./init.sql:/data/application/init.sql
        environment:
            MYSQL_ROOT_USER: root
            MYSQL_ROOT_PASSWORD: secret
            MYSQL_DATABASE: hue
            MYSQL_USER: root
            MYSQL_PASSWORD: secret
        networks:
        net_pet:
            ipv4_address: 172.27.1.14

    streamsets:
        image: streamsets/datacollector:3.13.0-latest
        ports:
            - "18630:18630"
        networks:
            net_pet:
            ipv4_address: 172.27.1.17  
    
    networks:
        net_pet:
        ipam:
            driver: default
            config:
            - subnet: 172.27.0.0/16
            